# -*- coding: utf-8 -*-
"""Project_Bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HHRNyIzCSmL9dEpkBi6NLgCPWEWbG7g-

## Using pre-trained BERT model to find tune itself with the project dataset

- Install needed packages
"""

# !pip install evaluate

"""- Log in to hugging face
- Token: ---
"""

from huggingface_hub import login

login()

"""- Connect to google drive"""

from google.colab import drive
drive.mount('/content/drive')

"""- Import data"""

import pandas as pd

df = pd.read_excel("/content/drive/MyDrive/Development/AI ML/Machine Learning/MyProjects/Misleading Headline Detection/Dataset/fake_new_dataset.xlsx")
df['text'] = df['title'] + " " + df['text']

df.head()

df.drop(['Unnamed: 0', 'title', 'subcategory'], axis=1, inplace=True)
df = df.dropna()

df.to_numpy()[100]



"""- Create dataset and tokenizer"""

from datasets import Dataset, load_dataset
from transformers import AutoTokenizer
from sklearn.model_selection import train_test_split

# dataset = load_dataset("yelp_review_full")
# dff = pd.DataFrame(dataset["train"])

# dff

dataset = Dataset.from_pandas(df)
tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")

def tokenize(examples):
    return tokenizer(examples["text"], padding=True, truncation=True, return_tensors='pt') # Changed to 'pt' for PyTorch Trainer

dataset = dataset.map(tokenize, batched=True)
dataset = dataset.train_test_split(test_size=0.2)

"""- Split data set into training and testing"""

small_train = dataset["train"]
small_eval = dataset["test"]

"""- Retrive the BERT base model. Direct the model to use GPU (cuda)"""

from transformers import AutoModelForSequenceClassification # Changed to AutoModelForSequenceClassification

# Assuming 2 labels (real/fake)
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2) # Added num_labels
model.to("cuda")

"""- Create compute metrics"""

import numpy as np
import evaluate

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    # convert the logits to their predicted class
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

"""- Create training arguments"""

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="fake_news_detection",
    eval_strategy="epoch",
    push_to_hub=True,
    num_train_epochs=5
)

"""- Create a trainer, get ready for fine-tuning phase
- API key: ---
"""

from transformers import Trainer, DataCollatorWithPadding

trainer = Trainer(
    model=model,
    processing_class=tokenizer,
    args=training_args,
    train_dataset=small_train,
    eval_dataset=small_eval,
    compute_metrics=compute_metrics,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer)
)
trainer.train()

model.push_to_hub("willphan1712/fake_news_detection")